{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9533b8be",
   "metadata": {},
   "source": [
    "# Correlation Matrix with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4d47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc85d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1482436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teste', 'pyproject.toml', 'report', 'weights', 'notebooks', 'requirements.txt', 'data', '.gitignore', 'README.md', 'Makefile', 'parkinson', '.git']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.listdir('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affa8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/home/labic/merlin_codes/dl/Parkinson-Diagnosis-Deeplearning')\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "    \n",
    "import parkinson\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28f7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDN = 50\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc346b",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3806cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:03<00:00, 43.06it/s]\n",
      "100%|██████████| 66/66 [00:01<00:00, 43.56it/s]\n"
     ]
    }
   ],
   "source": [
    "parkinson_data = parkinson.utils.data.batch_read('../data/PDs_columns')\n",
    "control_data = parkinson.utils.data.batch_read('../data/Controls_columns')\n",
    "\n",
    "control_atlas_data = parkinson.utils.data.select_atlas_columns(control_data, 'AAL3')\n",
    "parkinson_atlas_data = parkinson.utils.data.select_atlas_columns(parkinson_data, 'AAL3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3bd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_cache(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def compute_all_dtw_matrices(time_series_list, cache_path, final_cache_path=None, n_jobs=4):\n",
    "    if final_cache_path is None:\n",
    "        final_cache_path = cache_path.replace('.pkl', '_final.pkl')\n",
    "\n",
    "    # Carrega cache parcial existente, se houver\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Carregando cache existente de {cache_path}...\")\n",
    "        completed_results = load_cache(cache_path)\n",
    "    else:\n",
    "        completed_results = {}\n",
    "\n",
    "    total = len(time_series_list)\n",
    "    indices_to_process = [i for i in range(total) if i not in completed_results]\n",
    "\n",
    "    print(f\"Total: {total} pacientes\")\n",
    "    print(f\"Já computados: {len(completed_results)}\")\n",
    "    print(f\"Restantes: {len(indices_to_process)}\")\n",
    "\n",
    "    def process_and_save(idx):\n",
    "        print(f\"Iniciando paciente {idx}...\")\n",
    "        start_time = time.time()\n",
    "        ts = time_series_list[idx]\n",
    "        result = parkinson.utils.graph.compute_dtw_matrix(ts)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Paciente {idx} finalizado em {elapsed:.2f} segundos.\")\n",
    "        return idx, result\n",
    "\n",
    "    # Processa os pacientes em batches de n_jobs\n",
    "    for batch_start in range(0, len(indices_to_process), n_jobs):\n",
    "        batch_indices = indices_to_process[batch_start:batch_start + n_jobs]\n",
    "\n",
    "        print(f\"\\nProcessando batch {batch_start} a {batch_start + len(batch_indices) - 1}...\")\n",
    "\n",
    "        new_results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_and_save)(i) for i in batch_indices\n",
    "        )\n",
    "\n",
    "        # Atualiza o cache com os resultados do batch\n",
    "        for idx, res in new_results:\n",
    "            completed_results[idx] = res\n",
    "\n",
    "        # Salva o cache parcial após cada batch\n",
    "        save_cache(cache_path, completed_results)\n",
    "        print(f\"Cache parcial salvo após batch {batch_start}.\")\n",
    "\n",
    "    # Reordena os resultados finais\n",
    "    ordered_results = [completed_results[i] for i in range(total)]\n",
    "\n",
    "    # Salva o resultado final completo\n",
    "    save_cache(final_cache_path, completed_results)\n",
    "    print(f\"\\nResultado final salvo em {final_cache_path}\")\n",
    "\n",
    "    return ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0641156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando cache existente de cache_dtw_parkinson.pkl...\n",
      "Total: 153 pacientes\n",
      "Já computados: 153\n",
      "Restantes: 0\n",
      "\n",
      "Resultado final salvo em cache_dtw_parkinson_final.pkl\n"
     ]
    }
   ],
   "source": [
    "# com DTW\n",
    "parkinson_cache_path = \"cache_dtw_parkinson.pkl\"\n",
    "parkinson_correlation_matrix = compute_all_dtw_matrices(parkinson_atlas_data, parkinson_cache_path, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79fc39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando cache existente de cache_dtw_control.pkl...\n",
      "Total: 66 pacientes\n",
      "Já computados: 66\n",
      "Restantes: 0\n",
      "\n",
      "Resultado final salvo em cache_dtw_control_final.pkl\n"
     ]
    }
   ],
   "source": [
    "# com DTW\n",
    "control_cache_path = \"cache_dtw_control.pkl\"\n",
    "control_correlation_matrix = compute_all_dtw_matrices(control_atlas_data, control_cache_path, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# com pearson (descomentar se quiser usar pearson ao invés de DTW)\n",
    "# parkinson_correlation_matrix = [parkinson.utils.graph.compute_correlation_matrix(time_series) for time_series in parkinson_atlas_data]\n",
    "# control_correlation_matrix = [parkinson.utils.graph.compute_correlation_matrix(time_series) for time_series in control_atlas_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b34b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = parkinson.utils.data.concatenate_data(parkinson_correlation_matrix, control_correlation_matrix)\n",
    "y = parkinson.utils.data.concatenate_data([1 for _ in range(len(parkinson_data))], [0 for _ in range(len(control_data))])\n",
    "\n",
    "X, y = parkinson.utils.data.filter_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6e49e",
   "metadata": {},
   "source": [
    "### Split  \n",
    "- 60% treino\n",
    "- 20% validação\n",
    "- 20% teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81ee854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=RDN, stratify=y, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=RDN, stratify=y_trainval, shuffle=True)\n",
    "\n",
    "ros = RandomOverSampler(random_state=RDN)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f5cff",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c7da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = parkinson.utils.data.get_torch_dataloader(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "val_loader = parkinson.utils.data.get_torch_dataloader(X_val, y_val, batch_size=BATCH_SIZE)\n",
    "test_loader = parkinson.utils.data.get_torch_dataloader(X_test, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5fe79",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12eea3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = parkinson.NetworkModels.SimpleMLP(input_dim=X_train.shape[1], hidden_dim=16, output_dim=2)\n",
    "class_weights = parkinson.utils.data.get_torch_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b070166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16a618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6991  train-acc: 0.5000 | val-loss: 0.6559  val-acc: 0.7045:  18%|█▊        | 35/200 [00:11<00:54,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 36\n",
      "CPU times: user 8.75 s, sys: 9.36 s, total: 18.1 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = parkinson.utils.train.train(model, train_loader, val_loader, class_weights, device, N_EPOCHS,  PATIENCE,LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bd89ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics: {'acc': 0.7045454545454546, 'f1': 0.5824242424242424, 'recall': 0.7045454545454546, 'precision': 0.4963842975206612}\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calixto/miniconda3/envs/trabRN/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = parkinson.utils.train.evaluate(model, test_loader, device)\n",
    "print('Metrics:', metrics)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f7c4a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m val_loss =  \u001b[43ms\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m plt.plot(np.arange(\u001b[38;5;28mlen\u001b[39m(val_loss)), val_loss)\n",
      "\u001b[31mNameError\u001b[39m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_loss =  s['val_loss']\n",
    "plt.plot(np.arange(len(val_loss)), val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabRN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
